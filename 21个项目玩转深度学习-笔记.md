# 深度学习项目笔记

[TOC]

## 一、MNIST

### 1.1 关于MNIST

MNIST是手写数据集，每个图片由28X28个像素组成，加载方式:

```python
from tensorflow.examples.tutorials.mnist import input_data
import scipy.misc
import os
import numpy as np
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

save_dir = "D:\codes\\tensorflow-projects\MNIST_data\\raw\\"
if os.path.exists(save_dir) is False:
    os.makedirs(save_dir)
for i in range(20):
    image_array = mnist.train.images[i,:]
    image_array = image_array.reshape(28, 28)
    file_name = save_dir+'mnist_train_%d.jpg'%i
    scipy.misc.toimage(image_array, cmin=0.0, cmax=1.0).save(file_name)
    one_hot_label = mnist.train.labels[i, :]
    label = np.argmax(one_hot_label)
    # print('mnist_train_%d.jpg label: %d' % (i, label))
```

标签采用了独热（one-hot）表示，使用np.argmax可以得到其值。上面代码中，将MINIST数组数据通过

[scipy.misc.toimage()]: https://blog.csdn.net/c20081052/article/details/80917841

转化成图像。

### 1.2 Softmax回归

Softmax（多分类模型）是由logistic模型（二分类模型转化而来）。在MNIST中，计算每张图片属于10个类中的每个类的概率，概率最大的类别就是预测的结果。

Softmax函数的功能主要是将各个类别的“打分”转化为合理的概率值。例如一个样本可能属于三个类别，属于第一个类别的打分为a，属于第二个类别的打分为b，属于第三个类别的打分为c，打分越高则表示属于这个类别的可能性越大。但是打分本身不代表概率，打分可能为负数，也可能大于1，但是概率是在0~1。使用Softmax将打分（a, b, c）转化为属于各个类的概率值，如下。
$$
(\frac{e^a}{e^a + e^b + e^c}, \frac{e^b}{e^a + e^b + e^c}, \frac{e^c}{e^a + e^b + e^c})
$$
这三个值都在0~1之间，加起来等于1。

假设$x$ 是单个样本的特征，$W$ 、$b$ 是$Softmax$ 模型的参数，在MNIST数据集中$x$ 代表输入图片，是一个784（28X28）的向量，$W$ 是一个矩阵，形状为(784, 10)，$b$ 是10维的向量，10代表类别数，$Softmax$ 第一步是通过下面的公式计算各个类别的$Logit$ ：

$$Logit = W^T x+ b$$

$Logit​$ 是一个10维的向量，，在这里可以看作打分。下面使用$Softmax​$ 将打分转为各个类别的概率值：

$$y = Softmax(Logit)$$

整个模型可以表示为：

$$y = Softmax(W^Tx + b)$$

#### 1.2.1 使用Softmax

tf中的占位符和变量实际上都是$tensor$ 。

占位符不依赖于其他的$tensor$ ,它的值由用户自行传递给TF，通常用来存储样本数据和标签，None表示这一维的大小可以是任意的。

变量是指在极端过程中可以改变的值，通常用变量存储模型的参数。

模型的输出为y，实际的标签为y_，在$Softmax$ 回归模型中通常使用交叉熵衡量这种相似性。

构造损失函数之后，构造如何优化损失的方法，常用梯度下降。$tf.train.GradientDescentOptimizer$ 会对所有变量计算梯度，并更新变量的值。

在优化前必须先构造一个会话 $Session$ ，并在会话中对变量进行初始化操作。

```python
x = tf.placeholder(tf.float32, [None, 784])
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
# 模型输出
y = tf.nn.softmax(tf.matmul(x, W) + b)
# 实际标签
y_ = tf.placeholder(tf.float32, [None, 10])
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y)))
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
for _ in range(1000):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print(sess.run(accuracy, feed_dict={x:mnist.test.images, y_: mnist.test.labels}))
```



上面代码中几个函数的说明：

$tf.argmax()​$ ：提取数组中最大值的下表，本例中的下表代表标签活预测结果。

$tf.cast()$ ：将数组中的$True$转化成1，$False$转化成0。

$tf.raduce_-mean()$ ：求数组中所有元素的平均值。

#### 1.2.2使用卷积

CNN是局部连接、权重共享的深层前馈神经网络。

全连接前馈网络存在的问题：（1）参数太多（2）局部不变特征

局部不变特征：尺度缩放、平移旋转等操作不影响其语义信息，而全连接前馈网络很难提取到这些局部不变特征，一般需要进行数据增强来提高性能。

卷积网络结构特性：局部连接，权重共享，汇聚。

信号序列$x$和滤波器$w$的卷积定义为

$$y=w\bigotimes x​$$  , $\bigotimes​$ 表示卷积运算 

在图像处理中，常常用到二维卷积，给定一个图像$X \in \Bbb R ^{M \times N}$，滤波器$W \in \Bbb R ^ {m \times n}$，一般$m << M$， $n << N$，其卷积为：

$$y_{ij} = \sum_{u=1}^m \sum_{v=1}^n {w_{uv} \cdot x_{i-u+1, j-v+1} }​$$                 公式1.2.2.1

均值滤波（$mean filter$）把当前位置的像素值设为滤波窗口中所有的像素的平均值：$f_{uv} =  \frac {1} {mn}$，一幅图经过卷积操作后得到的结果称为特性映射（$Feature Map$）。在图像处理中，卷积经常作为特征提取的有效办法，通过卷积操作得到一组新的特征。在计算卷积核的过程中需要进行卷积核的翻转。在实现上，用互相关来代替卷积，从而减少不必要的操作和开销。

互相关是衡量两个序列相关性的函数，通常使用滑动窗口的点积计算实现。

$$y_{ij} = \sum_{u=1}^m \sum_{v=1}^n {w_{uv} \cdot x_{i+u-1, j+v-1} }$$                公式1.2.2.2

互相关和卷积的区别在于卷积核仅仅是否进行翻转，互相关也可以称为不翻转卷积。NN中使用卷积是为了进行特征抽取，是否进行翻转对信息抽取的能力无关。当卷积核是可学习的参数时，卷积和互相关时相互等价的。为了实现的方便，用互相关代替卷积，事实上，很多深度学习工具中的卷积操作都是互相关操作。

上面的公式可以表述为

$$Y=W\bigotimes X$$ 

其中 $Y \in \Bbb R^{{M-m+1},{N-n+1}}$为输出矩阵。







 